{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil Calibri;}{\f2\fnil\fcharset1 Segoe UI Symbol;}{\f3\fnil\fcharset2 Symbol;}}
{\*\generator Riched20 6.3.9600}\viewkind4\uc1 
\pard\sa200\sl240\slmult1\qc\ul\b\f0\fs28\lang9 Cleaning Data\par

\pard\sa200\sl240\slmult1\qj\ulnone\b0\fs22 *Raw Data is not always clean data to perform data analysis.\par
==>Before Analysis the data , should be cleaned\par
\ul\b *Raw data consists of:\ulnone\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl240\slmult1\qj\b0 Missing Values\ul\par
{\pntext\f3\'B7\tab}\ulnone Inappropriate column names with spaces,captalize,special characters\ul\par
{\pntext\f3\'B7\tab}\ulnone Outliers\ul\par
{\pntext\f3\'B7\tab}\ulnone Duplicate rows\ul\par
{\pntext\f3\'B7\tab}\ulnone untidy\ul\par
{\pntext\f3\'B7\tab}\ulnone Need to process columns\ul\par
{\pntext\f3\'B7\tab}\ulnone Column types can signal unexpected data values\ul\par

\pard\sa200\sl240\slmult1\qj\par
\b Steps to follow to clean the Data:\ulnone\par
Step 1: Undersatnding the Data\par
\b0\tab -> First understand the data and datatype\par
\tab ->To do : df.info()\par
\tab\tab * Find the datatypes of each column ,are they in correct data type or not\par
\tab ->Find the unique values in the columns \par
\tab\tab *To do : df['column_name'].\b value_counts\b0 (\b dropna\b0 =False )\par
\tab\tab *\b Dropna\b0 => False, returns the missing values\par
\tab -> Df.describe()\par
\tab\tab * Numeric info of all numeric columns\par
\b Step 2: Visualize the Data\par
\tab -> \b0 Helps to Spot the outliers and errors\par
\tab ->Helps Plan steps for data clean\par
\b\tab Bar Plots and Histograms :\par
\b0\tab\tab *\b Histograms \b0 are used for continous data like population over the years i.e \tab\tab\tab\tab how much population lies under each years\par
\b\tab\tab *Box plots \b0 for discrete data counts \par
\b\tab\tab =>Histograms:\par
\b0  \tab\tab\tab Helps to fins the outliers \par
\tab\tab\tab All outliers are not bad data points,some can be errors and some are \tab\tab\tab\tab valid points\par
\b\tab\tab\tab df.population.plot('hist') \par
\tab\tab => Box plots:\par
\tab\tab\tab Visualize basic the statistic summary like\par

\pard\sb100\sa100\tab\tab\tab ->\b0\fs24\lang1033 Boxplots are great when you have a numeric column that you \tab\tab\tab\tab want to compare across different categories,\b\fs22\lang9\par

\pard\sa200\sl240\slmult1\qj\tab\tab\tab ->\b0 Median,Min value, Max values,20th,50th,75th percentiles\par
\b\tab\tab\tab df.boxplot(column='Population',by='Year')\par
\tab\tab =>Scatter plots:\par

\pard\sb100\sa100\tab\tab\tab ->\b0\fs24\lang1033 To visualize two numeric columns, scatter plots are ideal,\b\fs22\lang9\par

\pard\sa200\sl240\slmult1\qj\tab\tab\tab ->\b0 Relationship between 2 numeric variables,\par
\tab\tab  \lang1033\tab\b\lang9 ->\b0 Flag potentially bad data ,\lang1033\par
\tab\tab\tab\b\lang9 ->\b0\lang1033 E\lang9 rors not found by looking at 1 variable\par
\par
\b Note:\par
\tab ** Histograms are used to visualize the single variables \par
\tab **Box plots are used to visualize the multiple  variables\par
\tab **Box plots are maily used to find the outlier of the data corresponding to the variables plotted\par
\par
Tidy Data: (arranged neatly and in order)\par
\tab ==> Tidying messy datasets:\par
\b0\tab\tab\f1\bullet  Column headers are values, not variable names.\par
\f0\lang1033\tab\tab\f1\lang9\bullet  Multiple variables are stored in one column.\par
\f0\lang1033\tab\tab\f1\lang9\bullet  Variables are stored in both rows and columns.\par
\f0\lang1033\tab\tab\f1\lang9\bullet  Multiple types of observational units are stored in the same table.\par
\f0\lang1033\tab\tab\f1\lang9\bullet  A single observational unit is stored in multiple tables\par
\b\f0\lang1033\tab ==>Principles Tidy data :\lang9\par
\b0\tab\tab * Each variable that you've measured be in exactly one column \par
\tab\tab * Each different observation should be in a different row\par
\b\tab\tab * Each type of observational unit forms a table\par
** pandas.melt(): \b0 is used to convert the columns values into variables, as tidy data should contains only column variables \par
\b  Eg: Untidy data\par
\b0  \tab\b Name\tab\tab treat1\tab\tab treat2\tab\b0\tab\par
\tab sns\tab\tab --\tab\tab 21\tab\tab\par
\tab ke\tab\tab 34\tab\tab 54\par
\tab ft\tab\tab 43\tab\tab 32\par
\b Tidy Data\b0\par
\b\tab Name\tab\tab Treatment\tab\tab T-values\tab\tab\par
\b0\tab sns\tab\tab treat1\tab\tab\tab --\par
\tab ke\tab\tab treat1\tab\tab\tab 34\par
\tab ft\tab\tab treat2\tab\tab\tab 43\par
\tab sns\tab\tab treat2\tab\tab\tab 21\par
\tab ke\tab\tab treat2\tab\tab\tab 54\par
\tab ft\tab\tab treat2\tab\tab\tab 32\par
\tab\b\tab\b0\par
Each column should contain the variables and each row should specify one abservation . This can be achived by using pandas method \b melt()\b0\par
\b parameters: Frame='DataSet_Frame',id_vars='represent the columns of the data you do not want to melt'\par
\tab\tab values_vars='On which columns melting should be done'\par
\tab\tab var_name='Can change the variable names'\par
\tab\tab value_name="Can chaneg the result"\par
\par
Pivot and pivot_table:\par
\tab ->\b0 Pivot is opposite to the melt, means converting the rows into columns.\par
\tab ->In some situations dataset is violating the tidy principles as each row is not specify one observation\par
\b\tab dataset.pivot(): Is used to convert the rows into columns\par
\tab parameters: \par
\tab\tab Index='column to be \b0 Index \b ',\par
\tab\tab columns='column data to be converted into seperate column '\par
\tab\tab value='Values to be corressponding to the columns '\par
Note: pivot can/t deal with the duplicate rows\b0\par
So\par
\b Pivot_table() is used to deal with the duplicates values\b0\par
\b dataset.pivot_table(): Is used to convert the rows into columns\par
\tab parameters: \par
\tab\tab Index='column to be \b0 Index \b ',\par
\tab\tab columns='column data to be converted into seperate column '\par
\tab\tab value='Values to be corressponding to the columns ',\par
\tab\tab aggfunc=np.mean\par
\par
\ul\par
\par
\ulnone\b0 -> Sometimes there will be a datasets which splits into dofferent datasets or files duw to large size or daily updating the data(stock dataset)\par
-> Combining the data can be done done in many ways, column wise ,row wise...\par
\b ROW wise:\par
->Data is combine by row wise, it should have same columns size.\par
Eg: Dataset 1\par
 \tab\tab ID\tab Name\tab\tab Age\tab\tab\par
\tab\tab 0\tab kkr\tab\tab 23\par
\tab\tab 1\tab rre\tab\tab 34\par
Dataset 2\tab\tab\par
\tab\tab ID\tab Name\tab\tab Age\tab\tab\par
\tab\tab 0\tab ERE\tab\tab 23\par
\tab\tab 1\tab WER\tab\tab 34\par

\pard\sa200\sl240\slmult1 RESULT:\par

\pard\sa200\sl240\slmult1\qj\tab\tab  ID\tab Name\tab\tab Age\tab\tab\par
\tab\tab 0\tab kkr\tab\tab 23\par
\tab\tab 1\tab rre\tab\tab 34\par
\tab\tab 0\tab ERE\tab\tab 23\par
\tab\tab 1\tab WER\tab\tab 34\par

\pard\sa200\sl240\slmult1 Note: \b0 Index are default , i.e 0 1 0 1\par
In pandas, concat() is used to row wise concatination.\par
\b pandas.concat([dataframe1,dataframe2],ignore_index=True)\par
\tab ignore_index=True, default is False--> \b0 It will ignore the dataset indexes, it will create a continous and unique indexes.\b\par

\pard\sa200\sl240\slmult1\qj\par
\par
Column wise:\par
\tab\b0 concatinatiing the columns of different datasets.\par
\tab Should have same length of rows.\par
\tab\b pandas.concat([df1,df2],axis=1,ignore_index=False)\par
\b0\par
\par
\b Note:\par
What if there are many data frames/data files, ?? \par
\tab ->Loading each data file/dataset requires more time.\par
\tab -> To Overcome the problem we use glob package.\par
\par
glob.glob()-> is used to fetch the similar files based on condition.\par
parameters: str: \par
i.e \tab glob.glob( '*.csv')--> fetches all csv files\par
\tab glob.glob('part_?.csv')-> fetches all cvs files with single character at the end either number or alphabet\par
Note:\tab ? represents any one character\b0\par
\par
\b\par
\par
'''\par
\par
concatenation Notes\par
\par
\par
'''\par
\par
\ul Merging the data:\ulnone\par
\b0\tab *concatenation is not only the way to combine data, there will be some situations where concatenation can't perform.\par
\b Eg: \tab Dataframe1 \tab\tab\tab\tab\tab\tab DataFrame2\par
\tab EmpName\tab loc\tab Sal\tab\tab\tab\tab Name\tab\tab Date of Joining\par
\b0\tab koushik\tab Bnglr\tab 20\tab\tab\tab\tab Rajesh\tab\tab 08/10/2016\b\par
\b0\tab Rahul\tab\tab Bnglr\tab 12\tab\tab\tab\tab Raj\tab\tab 08/10/2017\b\par
\b0\tab Raj\tab\tab Bnglr\tab 9\tab\tab\tab\tab Rahul\tab\tab 08/10/2016\par
In the above data frames, concatenate doen't work either row or column wise because they are not in same order\par
So merging has to perform, to combine data.\par
==> Merging is perform on the common column in the dataframes, in above DF, \b EmpName \b0 and \b Name\b0  are the common columns.\par
*pandas has\b  merge() function to perform merging \par
parameters:  \tab left   ->pandas.DataFrame,\b0 left  dataframe\b\par
\tab           \tab right->pandas.DataFrame,\b0 right dataframe\b\par
\tab\tab on    -> str,\b0 If two dataframes has common column with same column name\b\par
\tab\tab left_on->str,\b0 If two dataframes has common column with diff column name,left \tab\tab\tab\tab\tab DF column name\b\par
\tab\tab right_on->str,\b0 If two dataframes has common column with diff column \tab\tab\tab\tab\tab\tab name,right DF column name\par
\b Note:\b0 Either \b on\b0  or \b (left_on ,right_on) should be passed as arguments\par
\par
Merging has different types. They are \par
\tab 1.One to One\par
\tab 2. One to many/many to one\par
\tab 3.Many to Many\par
1. One to One  : \b0 values are unique\tab\tab\b\par
2. One to many/many to one\b0 : one of the values will be duplicated and recycled in the output. That is, one of the keys in the merge is not unique.\par
\b Site:\tab\tab\tab\tab\tab\tab\tab site\par
\b0     name    lat    long\tab\tab\tab\tab\tab     ident   site       dated\par
0   DR-1 -49.85 -128.57\tab\tab\tab\tab\tab 0    619   DR-1  1927-02-08\par
1   DR-3 -47.15 -126.72\tab\tab\tab\tab\tab 1    622   DR-1  1927-02-10\par
2  MSK-4 -48.87 -123.4\tab\tab\tab\tab\tab 2    734   DR-3  1939-01-07\par
\tab\tab\tab\tab\tab\tab\tab 3    735   DR-3  1930-01-12\par
\tab\tab\tab\tab\tab\tab\tab 4    751   DR-3  1930-02-26\par
\tab\tab\tab\tab\tab\tab\tab 5    752   DR-3         NaN\par
\tab\tab\tab\tab\tab\tab\tab 6    837  MSK-4  1932-01-14\par
\tab\tab\tab\tab\tab\tab\tab 7    844   DR-1  1932-03-22\par
m2o = pd.merge(left=site,right=visited,left_on='name',right_on='site')\par
\par
\b Output:\par
\b0      name    lat    long  ident   site       dated\par
    0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\par
    1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10\par
    2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22\par
    3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\par
    4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12\par
    5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26\par
    6   DR-3 -47.15 -126.72    752   DR-3         NaN\par
    7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\b\tab\tab\tab\tab\tab\par
\tab\tab\tab\tab\tab\tab\tab\par
\tab\tab\tab\tab\tab\tab\tab\b0\par
\b 3.Many to Many:\b0 Both DataFrames do not have unique keys for a merge\par
\b   c1  c2\tab\tab\tab\tab   c1  c2\b0\par
0  a   1\tab\tab\tab\tab 0  a  10\par
1  a   2\tab\tab\tab\tab 1  a   20\par
2  b   3\tab\tab\tab\tab 2  b   30\par
3  b   4\tab\tab\tab\tab 3  b  40\par
\par
output: \par
\b c1  c2_x  c2_y\b0\par
0  a     1    10\par
1  a     1    20\par
2  a     2    10\par
3  a     2    20\par
4  b     3    30\par
5  b     3    40\par
6  b     4    30\par
7  b     4    40\par
\par
\ul\b Cleaning data for analysis:\par
\ulnone Dealing with the datatypes:\par
-> \b0 We have some situtaions where data are not in proper datatype like string, int\par
-> So we have to convert from one data type to another data type for better analysis\par
like \tab\b str-int   ,int-str,\tab str-float\par
\ul Categorical data\ulnone\par
\b0\tab *Converting categorical data to \lquote category\rquote  dtype:\par
\tab * Can make the DataFrame smaller in memory\par
\tab * Can make them be utilized by other Python libraries for analysis\par
\tab\b df.column_name.astype('catogory')\par
\par
Object-Int/float\par
pd.to_numeric() \b0 function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. You can either use the techniques to do some exploratory data analysis and find the bad value, or you can choose to ignore or \b coerce \b0 the value into a missing value, NaN.\par
\b Eg:  pd.to_numeric(df['age'],errors='coerce' )\par
\par
\ul Regular Expression:\par
\ulnone\b0\f2\lang1033\u9679?\f0  A formal way of specifying a pa!ern\par
\f2\u9679?\f0  Sequence of characters\par
\f2\u9679?\f0  Pattern matching \par
\par
Using regular expressions\par
\f2\u9679?\f0  Compile the pa!ern\par
\f2\u9679?\f0  Use the compiled pa!ern to match values\par
\f2\u9679?\f0  This lets us use the pa!ern over and over again\par
\f2\u9679?\f0  Useful since we want to match values  down\par
a column of values\par
\par
\b Import re\par
pattern=re.compile(' ^ \\d \{2\}-\\d*$')\par
bool( pattern.match(12-32))==> True\par
\par
re.findall( 'pattern Eg: \\d',' String to be compared Eg  Hello 2 and 3'  ) -returns the matched values\par
\par
Duplicates and Missing values:\par
\b0 Duplicates: drop_duplicates() is used to drop the duplicates.\par
\ul\b\fs28 Pandas:\ulnone\fs22\par
\tab df_set.drop_duplicates()\par
Missing Values:\par
\b0 Missing values can be handled in many ways.\par
\tab 1.leave as it \par
\tab 2.Drop missing values --> \b df.dropna()\b0\par
\tab 3.Fill missing values with user choice or statistical like mean,median-->\b  \tab\tab df[['income','age']].fillna(0)\b0\par
\b Note\b0 : Droping the missing values can be more dangerous as if one column contains missing values, entire row is droped \par
*\b Median is a be!er statistic in the presence of outliers\par
\par
\par
Assert Statement:\b0\par
\f2\u9679?\f0  Programmatically vs visually checking\par
\f2\u9679?\f0  If we drop or fill NaNs, we expect 0 missing values\par
\f2\u9679?\f0  We can write an assert statement to verify this\par
\f2\u9679?\f0  We can detect early warnings and errors\par
\f2\u9679?\f0  This gives us confidence that our code is running correctly\par
\par
\b notnull().all()==> returns True if no nan values are present \par
isnull().all()==>returns True if it contains nan values\par
\par
\fs28 Exploratory analysis Steps need to perform on dataset:\par
step 1:\b0  U\fs22 se pandas methods such as .\b head(), \b0 .\b info\b0 (), and .\b describe()\b0 , and DataFrame attributes like .columns and .shape to explore it.\par
\b Step 2: Visualizing your data\par
\par
To Find the Uniques values in entire dataset:\par
df['Life expectancy'].value_counts()[0]--> \b0 returns the first values, the results are in decending order, if any repeating value counts comes at the first place\fs18\par
\b\fs22\par
\par
\b0\par
\par
\par
\par
\tab\par
\b\par
\par
\b0\par
\par
\par
\b\tab\tab\tab\par
\tab\tab\b0\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\par
\ul\par
}
 